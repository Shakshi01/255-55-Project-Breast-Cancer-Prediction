# -*- coding: utf-8 -*-
"""V3.0 255: Project Breast Cancer [With IMAGES]

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZixucTgUQvAPHF-cn150hLaDq7Ty5T2
"""

import pandas as pd
# Read the CSV file
df0 = pd.read_csv('train.csv')
# source: https://www.kaggle.com/competitions/rsna-breast-cancer-detection/data?select=train.csv
df0

!pip install kaggle



"""#### Go to the "Account" tab in your profile (https://www.kaggle.com/YOUR_USERNAME/account).

#### Scroll down to the "API" section and click on the "Create New API Token" button. This will download a file called kaggle.json. Keep it safe as it contains your API credentials.
"""

import os

def upload_file():
    file_path = input("Enter the path to the file you want to upload: ")
    
    if not os.path.isfile(file_path):
        print("The provided path is not a file. Please try again.")
        return None
    
    file_name = os.path.basename(file_path)
    
    with open(file_path, "rb") as f:
        file_content = f.read()
    
    print(f"Uploaded file: {file_name}")
    return file_name, file_content

# okimport os

# downloads_folder = os.path.join(os.path.expanduser("~"), "Downloads")
# kaggle_file_path = os.path.join(downloads_folder, "kaggle.json")
# print(kaggle_file_path)

# Upload your kaggle.json file to Google Colab:
# A file upload dialog will appear. Select the kaggle.json file you downloaded earlier from your local system.
from google.colab import files
files.upload()

# file_name, file_content = upload_file()

# import os
# print(os.getcwd())

# Move the kaggle.json file to the correct location and set the appropriate permissions:
# FOR GOOGLE COLLAB
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# kaggle_file_path = "/Users/spartan/Downloads/kaggle.json"
# !mkdir -p ~/.kaggle
# !mv {kaggle_file_path} ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

# Make sure to use the dataset's API command found on the dataset page under the "Three Dots" menu > "Copy API command":

# source: https://www.kaggle.com/datasets/theoviel/rsna-breast-cancer-256-pngs
!kaggle datasets download -d theoviel/rsna-breast-cancer-256-pngs #[1 GB]

# source: https://www.kaggle.com/datasets/theoviel/rsna-breast-cancer-512-pngs
# !kaggle datasets download -d theoviel/rsna-breast-cancer-512-pngs #[4 GB]

# source: https://www.kaggle.com/datasets/theoviel/rsna-breast-cancer-1024-pngs
# !kaggle datasets download -d theoviel/rsna-breast-cancer-1024-pngs #[14 GB]

# Unzip the downloaded dataset:
!unzip rsna-breast-cancer-256-pngs.zip

"""Dataset Description

## Note: The dataset  contains radiographic breast images of female subjects.
# The goal: is to identify cases of breast cancer in mammograms from screening exams. 

It is important to identify cases of cancer for obvious reasons, but false positives also have downsides for patients. As millions of women get mammograms each year, a useful machine learning tool could help a great many people.
"""

from IPython.display import Image
Image('/content/10011_1031443799.png')

"""When working with images in a machine learning task like identifying cases of breast cancer in mammograms, there are a variety of features that can be extracted from the images. Here are some common types of features that can be extracted:

[1] Pixel-based features: This involves extracting features directly from the pixels of the images, such as color intensity, texture, and shape.

[2] Statistical features: This involves calculating statistical properties of the pixel values in the images, such as mean, variance, and skewness.

[3] Structural features: This involves analyzing the structure of the images, such as identifying edges, shapes, and patterns.

[4] Domain-specific features: This involves extracting features that are specific to the domain of breast cancer detection, such as the presence of masses, calcifications, or architectural distortions.

[5] Deep learning features: This involves using pre-trained convolutional neural networks (CNNs) to extract features from the images, which can be used as input to a machine learning model.

### Some specific examples of features that could be extracted from mammogram images include:

[1] Mean and standard deviation of pixel values in the image

[2] Histogram of pixel values in the image

[3] Texture features, such as gray-level co-occurrence matrix (GLCM) features or gray-level run length matrix (GLRLM) features

[4] Shape features, such as the circularity or compactness of masses or the branching structure of ductal carcinoma in situ (DCIS)

[5] Features related to the distribution and density of microcalcifications, such as their size, shape, and clustering

[6] Deep learning features extracted from pre-trained CNNs such as VGG, ResNet, or Inception models.

Ultimately, the specific features that are most useful for identifying breast cancer in mammograms will depend on the particular dataset and the goals of the machine learning task. It is important to carefully analyze the images and experiment with different feature extraction methods to determine which features are most predictive of cancer.





Regenerate response

"""

# import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

# Following code uses the opencv and tensorflow packages, so make sure to install them in your Google Colab environment
!pip install tensorflow
!pip install opencv-python-headless tensorflow
!pip install scikit-image
!pip install opencv-python

"""#### This code will create a DataFrame containing 

#### the mean and standard deviation of pixel values, histogram of pixel values, GLCM features, and deep learning features extracted using the VGG16 model and extract additional features like color intensity, variance, skewness, and edges

#### for all images in the /content folder. 

#### The extracted features will be saved to a CSV file
"""

import os
import cv2
import pandas as pd
import numpy as np
from skimage.feature import greycomatrix, greycoprops, canny
from skimage.measure import regionprops, label
from skimage.morphology import closing, square, remove_small_objects
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing import image as tf_image
from scipy.stats import skew

# Function to calculate GLCM features
def glcm_features(gray_image):
    glcm = greycomatrix(gray_image, [1], [0],  symmetric=True, normed=True)
    contrast = greycoprops(glcm, 'contrast')[0, 0]
    dissimilarity = greycoprops(glcm, 'dissimilarity')[0, 0]
    homogeneity = greycoprops(glcm, 'homogeneity')[0, 0]
    energy = greycoprops(glcm, 'energy')[0, 0]
    correlation = greycoprops(glcm, 'correlation')[0, 0]
    return [contrast, dissimilarity, homogeneity, energy, correlation]

# Function to load VGG16 model
# def load_vgg16_model():
#     model = VGG16(weights='imagenet', include_top=False)
#     return model

# Function to extract deep learning features using VGG16
def extract_vgg16_features(model, img):
    img = cv2.resize(img, (224, 224))
    img = tf_image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)
    print("Input image shape:", img.shape)
    features = model.predict(img)
    return features.flatten()

from skimage.filters import threshold_otsu

# Function to calculate shape features (circularity and compactness)
def shape_features(img):
    # Apply binary threshold
    thresh = threshold_otsu(img)
    binary_img = img > thresh

    # Label the binary image
    labeled_img = label(binary_img)
    properties = regionprops(labeled_img)
    
    if not properties:
        return [None, None]
    
    area = properties[0].area
    perimeter = properties[0].perimeter
    compactness = (perimeter**2) / (4 * np.pi * area) if area > 0 and perimeter > 0 else None
    circularity = 4 * np.pi * area / (perimeter**2) if area > 0 and perimeter > 0 else None
    
    return [compactness, circularity]

# Function to extract microcalcification features (size, shape, and clustering)
def microcalcification_features(img):
    binary_img = closing(img > np.mean(img), square(3))
    labeled_img = label(binary_img)
    remove_small_objects(labeled_img, 10, in_place=True)
    
    properties = regionprops(labeled_img)
    areas = [prop.area for prop in properties]
    solidity = [prop.solidity for prop in properties]
    
    if not areas:
        return [None, None, None]
    
    mean_size = np.mean(areas)
    mean_solidity = np.mean(solidity)
    clustering = len(areas) / np.sum(areas)
    
    return [mean_size, mean_solidity, clustering]

# # Load VGG16 model
# vgg16_model = load_vgg16_model()

# Initialize an empty list to store image features
image_features = []

# Iterate through images in the /content folder
images_folder = "/content"  # Replace this with the path to your folder containing the image files
# image_count = 0

for file_name in os.listdir(images_folder):
  # if image_count >= 15000:
  #       break
  # for file_name in os.listdir('/content'):
  if file_name.endswith('.png'):
      # Read the image
      img = cv2.imread(os.path.join(images_folder, file_name))

      # Convert image to grayscale
      gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
      
      # Calculate color intensity
      color_intensity = np.mean(img, axis=(0, 1))

      # Calculate variance
      variance = np.var(gray_img)

      # Calculate skewness
      skewness = skew(gray_img.flatten())

      # Calculate edges
      edges = canny(gray_img)

      # Extract features
      mean_pixel_value = np.mean(gray_img)
      std_pixel_value = np.std(gray_img)
      #histogram = cv2.calcHist([gray_img], [0], None, [256], [0, 256]).flatten()
      glcm = glcm_features(gray_img)
      #vgg16_features = extract_vgg16_features(vgg16_model, img)
      shape = shape_features(gray_img)
      microcalcifications = microcalcification_features(gray_img)

      # Append features to the list
      image_features.append([file_name] + list(color_intensity) + [mean_pixel_value, std_pixel_value, variance, skewness] + \
                            #list(histogram) + \
                            glcm + \
                            #list(vgg16_features) + \
                            shape + microcalcifications + [np.sum(edges)])

      # # Increment image counter
      # image_count += 1

# Create a DataFrame from the extracted features
df = pd.DataFrame(image_features, columns=['file_name', 'color_intensity_r', 'color_intensity_g', 'color_intensity_b', 'mean_pixel_value', 'std_pixel_value', 'variance', 'skewness'] +\
                  #[f'hist_{i}' for i in range(256)] +\
                  ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation'] +\
                  #[f'vgg16_{i}' for i in range(vgg16_features.shape[0])] +\
                  ['compactness', 'circularity', 'mean_microcalcification_size', 'mean_microcalcification_solidity', 'microcalcification_clustering'] + ['edge_count'])

# Save the DataFrame to a CSV file
df.to_csv('image_features.csv', index=False)

df

df

# save file as csv
df.to_csv('image_features.csv', index=False)

# save file as csv
df.to_csv('image_features.csv', index=False)

# download file to local machine
from google.colab import files

file_name = 'image_features.csv'
df.to_csv(file_name, index=False)
files.download(file_name)

import pandas as pd
# Read the CSV file into a pandas DataFrame
df1 = pd.read_csv('image_features.csv')
df1

df0 = pd.read_csv("train.csv")
df0

import pandas as pd

# Read the dataset
df0 = pd.read_csv('train.csv')

# Create the new "file_name" column
df0['file_name'] = df0['patient_id'].astype(str) + '_' + df0['image_id'].astype(str) + '.png'

# Move the "file_name" column to the first position in the DataFrame
cols = df0.columns.tolist()
cols.insert(0, cols.pop(cols.index('file_name')))
df0 = df0.reindex(columns=cols)

# Check the updated DataFrame
df0

import pandas as pd

# Merge the dataframes on the "file_name" column
merged_df = pd.merge(df0, df1, on='file_name')

# Check the merged DataFrame
print(merged_df.shape)
merged_df

# save file as csv
merged_df.to_csv('merged_features.csv', index=False)

# download file to local machine
from google.colab import files

file_name = 'merged_features.csv'
merged_df.to_csv(file_name, index=False)
files.download(file_name)

import pandas as pd
merged_df = pd.read_csv('merged_features.csv')
merged_df

# # Drop the first 4 columns
# merged_df = merged_df.drop(merged_df.columns[:4], axis=1)

# # Drop the 'machine_id' column
merged_df = merged_df.drop('file_name', axis=1)
merged_df = merged_df.drop('patient_id', axis=1)
merged_df = merged_df.drop('image_id', axis=1)

# # Display the updated DataFrame
merged_df

"""# Data Analysis"""

df = merged_df
# Display descriptive statistics for each feature
with pd.option_context('display.max_columns', None):
    print(df.describe())

# Count NaN values in each column
nan_counts = df.isna().sum()

# Display the NaN counts for each feature
for feature, count in nan_counts.iteritems():
    print(f"NaN count in {feature}: {count}")

# Count zero values in each feature
zero_counts = (df == 0).sum()

# Display the zero counts for each feature
for feature, count in zero_counts.iteritems():
    print(f"Zero count in {feature}: {count}")

"""# data type"""

# Check data types of each column
print(df.dtypes)

# Function to determine the type of a column
def identify_type(column):
    if column.dtype in ['float64', 'int64']:
        if len(column.unique()) < 10:
            return 'numerical_categorical'
        else:
            return 'continuous'
    elif column.dtype == 'object':
        return 'textual_categorical'
    else:
        return 'unknown'

# Print the type of each column
for col_name, col_data in df.iteritems():
    print(f"{col_name}: {identify_type(col_data)}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Print the type of each column and create distribution plots for categorical variables
numerical_categorical_cols = []
textual_categorical_cols = []
for col_name, col_data in df.iteritems():
    col_type = identify_type(col_data)
    #print(f"{col_name}: {col_type}")

    if col_type == 'numerical_categorical':
        numerical_categorical_cols.append(col_name)
    elif col_type == 'textual_categorical':
        textual_categorical_cols.append(col_name)

# Plot pie charts for numerical_categorical variables
for col_name in numerical_categorical_cols:
    counts = df[col_name].value_counts()
    fig, ax = plt.subplots()
    wedges, texts, autotexts = ax.pie(counts, autopct='%1.1f%%', textprops=dict(color="w"))
    ax.legend(wedges, counts.index, title=f'{col_name} Labels', loc='center left', bbox_to_anchor=(1, 0, 0.5, 1))
    plt.setp(autotexts, size=8, weight='bold')
    ax.set_title(f'Pie Chart of {col_name}')
    plt.show()

# Plot count plots for textual_categorical variables
for col_name in textual_categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x=col_name)
    plt.title(f'Count Plot of {col_name}')
    plt.show()

"""# EDA"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Set up the figure and axes for subplots
fig, axes = plt.subplots(11, 3, figsize=(20, 60))

# Bar plots for categorical variables
#sns.countplot(x='site_id', data=df, ax=axes[0, 0])
sns.countplot(x='laterality', data=df, ax=axes[0, 1])
sns.countplot(x='view', data=df, ax=axes[0, 2])
sns.countplot(x='cancer', data=df, ax=axes[1, 0])
sns.countplot(x='biopsy', data=df, ax=axes[1, 1])
sns.countplot(x='invasive', data=df, ax=axes[1, 2])
sns.countplot(x='BIRADS', data=df, ax=axes[2, 0])
sns.countplot(x='implant', data=df, ax=axes[2, 1])
sns.countplot(x='density', data=df, ax=axes[2, 2])

# Histograms for continuous variables
df['age'].plot(kind='hist', ax=axes[3, 0])
#df['machine_id'].plot(kind='hist', ax=axes[3, 1])
df['color_intensity_r'].plot(kind='hist', ax=axes[3, 2])
df['color_intensity_g'].plot(kind='hist', ax=axes[4, 0])
df['color_intensity_b'].plot(kind='hist', ax=axes[4, 1])
df['mean_pixel_value'].plot(kind='hist', ax=axes[4, 2])
df['std_pixel_value'].plot(kind='hist', ax=axes[5, 0])
df['variance'].plot(kind='hist', ax=axes[5, 1])
df['skewness'].plot(kind='hist', ax=axes[5, 2])
df['contrast'].plot(kind='hist', ax=axes[6, 0])
df['dissimilarity'].plot(kind='hist', ax=axes[6, 1])
df['homogeneity'].plot(kind='hist', ax=axes[6, 2])
df['energy'].plot(kind='hist', ax=axes[7, 0])
df['correlation'].plot(kind='hist', ax=axes[7, 1])
df['compactness'].plot(kind='hist', ax=axes[7, 2])
df['circularity'].plot(kind='hist', ax=axes[8, 0])
df['mean_microcalcification_size'].plot(kind='hist', ax=axes[8, 1])
df['mean_microcalcification_solidity'].plot(kind='hist', ax=axes[8, 2])
df['microcalcification_clustering'].plot(kind='hist', ax=axes[9, 0])
df['edge_count'].plot(kind='hist', ax=axes[9, 1])

# Box plots for continuous variables by cancer
sns.boxplot(x='cancer', y='age', data=df, ax=axes[10, 0])
#sns.boxplot(x='cancer', y='machine_id', data=df, ax=axes[10, 1])

# Adjust spacing between subplots
fig.tight_layout()

# Show the plots
plt

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming merged_df is the DataFrame with all the features

# Compute the correlation matrix between the features
correlation_matrix = merged_df.corr()

# Plot the correlation matrix with a heatmap
plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", square=True)
plt.title("Correlation Matrix of Features")
plt.show()

# Plot the distribution of the target variable 'cancer'
sns.countplot(x="cancer", data=merged_df)
plt.title("Distribution of Cancer")
plt.show()

# Plot the distribution of the categorical variables with respect to 'cancer'
categorical_features = ['laterality', 'view', 'implant', 'density', 'difficult_negative_case']

for feature in categorical_features:
    plt.figure(figsize=(10, 6))
    sns.countplot(x=feature, data=merged_df, hue="cancer")
    plt.title(f"Distribution of {feature} with respect to Cancer")
    plt.show()

# Plot the distribution of the continuous variables with respect to 'cancer'
continuous_features = [col for col in merged_df.columns if col not in categorical_features + ['cancer']]

for feature in continuous_features:
    plt.figure(figsize=(10, 6))
    sns.histplot(data=merged_df, x=feature, hue="cancer", kde=True)
    plt.title(f"Distribution of {feature} with respect to Cancer")
    plt.show()

"""# Handle NAN values
This code uses K-Nearest Neighbors (KNN) imputation for both continuous and categorical columns. Note that KNN imputation is a more computationally expensive method than simple imputations like mean, median, or mode, and its performance may depend on the specific characteristics of the data. Make sure to validate the performance of KNN imputation on your dataset before using it in a production setting.
"""

# missing / Nan values BEFORE imputation
print(df.isna().sum())

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

# Function to perform KNN imputation for categorical columns
def knn_impute_categorical(df, column, knn_imputer):
    column_encoded = pd.get_dummies(df[column])
    column_imputed = knn_imputer.fit_transform(column_encoded)
    column_imputed_df = pd.DataFrame(column_imputed, columns=column_encoded.columns)
    return column_imputed_df.idxmax(axis=1)

# Function to perform KNN imputation for continuous columns
def knn_impute_continuous(df, column, knn_imputer):
    column_imputed = knn_imputer.fit_transform(df[column].values.reshape(-1, 1))
    return np.round(column_imputed).reshape(-1)

# Set up KNN imputer
knn_imputer = KNNImputer(n_neighbors=3, weights='distance')

# Iterate through columns and apply KNN imputation
for col in df.columns:
    if col in ['cancer', 'biopsy', 'invasive', 'BIRADS', 'implant']:
        df[col] = knn_impute_categorical(df, col, knn_imputer)
    elif col in ['age', 'color_intensity_r', 'color_intensity_g', 'color_intensity_b', 'mean_pixel_value', 'std_pixel_value', 'variance', 'skewness', 'contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'compactness', 'circularity', 'mean_microcalcification_size', 'mean_microcalcification_solidity', 'microcalcification_clustering', 'edge_count']:
        df[col] = knn_impute_continuous(df, col, knn_imputer).flatten()
    elif col in ['laterality', 'view', 'density', 'difficult_negative_case']:
        df[col] = knn_impute_categorical(df, col, knn_imputer)

# Check if there are any remaining missing values
print(df.isna().sum())

# Save the scaled DataFrame as a CSV file
df.to_csv('df_imputed.csv', index=False)

# Read the CSV file back as a DataFrame
df_imputed = pd.read_csv('df_imputed.csv')

# Display the first 5 rows of the DataFrame
df_imputed



"""# Label Encoding"""

# Display Dataframe AFTER Encoding DataFrame

"""# Data Scaling"""

# # 5 model training

"""# MODEL TRAINING"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import CategoricalNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, cohen_kappa_score, matthews_corrcoef, log_loss

# Read the dataset
df = df_imputed

# Preprocessing
laterality_encoder = LabelEncoder()
view_encoder = LabelEncoder()
density_encoder = LabelEncoder()
difficult_negative_case_encoder = LabelEncoder()

df['laterality'] = laterality_encoder.fit_transform(df['laterality'].astype(str))
df['view'] = view_encoder.fit_transform(df['view'].astype(str))
df['density'] = density_encoder.fit_transform(df['density'].astype(str))
df['difficult_negative_case'] = difficult_negative_case_encoder.fit_transform(df['difficult_negative_case'].astype(str))

# Scaling continuous features
continuous_features = ['age', 'color_intensity_r', 'color_intensity_g', 'color_intensity_b', 'mean_pixel_value', 'std_pixel_value', 'variance', 'skewness', 'contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'compactness', 'circularity', 'mean_microcalcification_size', 'mean_microcalcification_solidity', 'microcalcification_clustering', 'edge_count']
minmax_scaler = MinMaxScaler()
scaled_data_minmax = minmax_scaler.fit_transform(df[continuous_features])
scaled_df_minmax = pd.DataFrame(scaled_data_minmax, columns=continuous_features)
df[continuous_features] = scaled_df_minmax

df

# Train-test split
X = df.drop('cancer', axis=1)
y = df['cancer']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 1: Train the 5 models
models = [
    ('Logistic Regression', LogisticRegression()),
    ('Decision Tree', DecisionTreeClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('K-Nearest Neighbour', KNeighborsClassifier()),
    ('Categorical NB', CategoricalNB())
]

model_metrics = []

for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    confusion = confusion_matrix(y_test, y_pred)
    cohen_kappa = cohen_kappa_score(y_test, y_pred)
    matthews_corr = matthews_corrcoef(y_test, y_pred)
    log_loss_score = log_loss(y_test, y_pred_proba)

    metrics = {
        'name': name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'confusion': confusion,
        'cohen_kappa': cohen_kappa,
        'matthews_corr': matthews_corr,
        'log_loss': log_loss_score
    }
    model_metrics.append(metrics)

model_metrics

# Step 2: Find top 3 models
# You can assign weights to each performance metric and compute the weighted average score for each model
# For illustration, we assign equal weights to all metrics
weights = {
    'accuracy': 1,
    'precision': 1,
    'recall': 1,
    'f1': 1,
    'roc_auc': 1,
    'cohen_kappa': 1,
    'matthews_corr': 1,
    'log_loss': -1  # negative weight because lower log_loss is better
}

for metrics in model_metrics:
    weighted_score = 0
    for key, weight in weights.items():
        if key != 'confusion':
            weighted_score += metrics[key] * weight
    metrics['weighted_score'] = weighted_score

model_metrics.sort(key=lambda x: x['weighted_score'], reverse=True)
top3_models = model_metrics[:3]
top3_models

from sklearn.ensemble import VotingClassifier
# Step 3: Retrain top 3 models and ensemble model
# Create a dictionary for easy model lookup
model_dict = {name: model for name, model in models}

# Select the top 3 models from the dictionary
top3_model_list = [(model_metrics['name'], model_dict[model_metrics['name']]) for model_metrics in top3_models]

# Create the ensemble model
ensemble_model = VotingClassifier(estimators=top3_model_list, voting='soft')

# Add the ensemble model to the list of models
models.append(('Ensemble', ensemble_model))

# Train the ensemble model
ensemble_model.fit(X_train, y_train)

# Step 4: Take a weighted average of each performance metric from the top 3 models and the ensemble model
model_metrics = []

for name, model in models:
    if name in [model_metrics['name'] for model_metrics in top3_models] or name == 'Ensemble':
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        confusion = confusion_matrix(y_test, y_pred)
        cohen_kappa = cohen_kappa_score(y_test, y_pred)
        matthews_corr = matthews_corrcoef(y_test, y_pred)
        log_loss_score = log_loss(y_test, y_pred_proba)

        metrics = {
            'name': name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'confusion': confusion,
            'cohen_kappa': cohen_kappa,
            'matthews_corr': matthews_corr,
            'log_loss': log_loss_score
        }
        model_metrics.append(metrics)


model_metrics

# Convert the model_metrics list to a DataFrame
model_metrics_df = pd.DataFrame(model_metrics)

# Display the model_metrics DataFrame
model_metrics_df



"""# Because of highly imbalanced datasets, everything is looking PERFECT

# So, let's train in batches to maintain equal ratio of Cancer and Non-Cancer datasets 
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import CategoricalNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, cohen_kappa_score, matthews_corrcoef, log_loss

# Read the dataset
df = df_imputed

# Preprocessing
laterality_encoder = LabelEncoder()
view_encoder = LabelEncoder()
density_encoder = LabelEncoder()
difficult_negative_case_encoder = LabelEncoder()

df['laterality'] = laterality_encoder.fit_transform(df['laterality'].astype(str))
df['view'] = view_encoder.fit_transform(df['view'].astype(str))
df['density'] = density_encoder.fit_transform(df['density'].astype(str))
df['difficult_negative_case'] = difficult_negative_case_encoder.fit_transform(df['difficult_negative_case'].astype(str))

# Scaling continuous features
continuous_features = ['age', 'color_intensity_r', 'color_intensity_g', 'color_intensity_b', 'mean_pixel_value', 'std_pixel_value', 'variance', 'skewness', 'contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'compactness', 'circularity', 'mean_microcalcification_size', 'mean_microcalcification_solidity', 'microcalcification_clustering', 'edge_count']
minmax_scaler = MinMaxScaler()
scaled_data_minmax = minmax_scaler.fit_transform(df[continuous_features])
scaled_df_minmax = pd.DataFrame(scaled_data_minmax, columns=continuous_features)
df[continuous_features] = scaled_df_minmax

# Train-test split
X = df.drop('cancer', axis=1)
y = df['cancer']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 1: Train the 5 models
models = [
    ('Logistic Regression', LogisticRegression()),
    ('Decision Tree', DecisionTreeClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('K-Nearest Neighbour', KNeighborsClassifier()),
    ('Categorical NB', CategoricalNB())
]

"""# Step 1: To create equal batches of cancer and non-cancer cases,"""

import numpy as np

# Create separate datasets for cancer and non-cancer cases
cancer_df = df[df['cancer'] == 1]
non_cancer_df = df[df['cancer'] == 0]

# Calculate the number of batches needed
num_batches = int(non_cancer_df.shape[0] / cancer_df.shape[0])

# Create a list to store balanced datasets
balanced_datasets = []

# Create balanced datasets by sampling from non-cancer cases
for _ in range(num_batches):
    non_cancer_sample = non_cancer_df.sample(n=cancer_df.shape[0], random_state=42)
    balanced_df = pd.concat([cancer_df, non_cancer_sample], axis=0)
    balanced_datasets.append(balanced_df)

#balanced_datasets

"""Now you have a list of balanced datasets in balanced_datasets. You can iterate through this list and train your models using each balanced dataset. Modify the model training code to train on each balanced dataset:"""

# Initialize model metrics
model_metrics = []

for i, balanced_df in enumerate(balanced_datasets):
    # Train-test split
    X = balanced_df.drop('cancer', axis=1)
    y = balanced_df['cancer']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the models
    for name, model in models:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        confusion = confusion_matrix(y_test, y_pred)
        cohen_kappa = cohen_kappa_score(y_test, y_pred)
        matthews_corr = matthews_corrcoef(y_test, y_pred)
        log_loss_score = log_loss(y_test, y_pred_proba)

        metrics = {
            'batch': i,
            'name': name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'confusion': confusion,
            'cohen_kappa': cohen_kappa,
            'matthews_corr': matthews_corr,
            'log_loss': log_loss_score
        }
        model_metrics.append(metrics)

model_metrics_df = pd.DataFrame(model_metrics)
model_metrics_df

"""# Step 2: Find top 3 models
First, let's modify the code to calculate the average performance for each model across all batches. Then, we can find the top 3 models based on the weighted average score:
"""

# Calculate the average performance for each model across all batches
model_avg_metrics = {}
for metrics in model_metrics:
    name = metrics['name']
    if name not in model_avg_metrics:
        model_avg_metrics[name] = {key: 0 for key in weights.keys()}
        model_avg_metrics[name]['count'] = 0

    for key in weights.keys():
        model_avg_metrics[name][key] += metrics[key]
    model_avg_metrics[name]['count'] += 1

for name, metrics in model_avg_metrics.items():
    for key in weights.keys():
        metrics[key] /= metrics['count']

# Compute the weighted average score for each model
for name, metrics in model_avg_metrics.items():
    weighted_score = 0
    for key, weight in weights.items():
        weighted_score += metrics[key] * weight
    metrics['weighted_score'] = weighted_score

# Sort the models by weighted score and select the top 3
sorted_models = sorted(model_avg_metrics.items(), key=lambda x: x[1]['weighted_score'], reverse=True)
top3_models = [model for model, metrics in sorted_models[:3]]
top3_models

from sklearn.ensemble import VotingClassifier

# Step 3: Retrain top 3 models and create ensemble model
top3_model_instances = [(name, model) for name, model in models if name in top3_models]
ensemble_model = VotingClassifier(estimators=top3_model_instances, voting='soft')

# Retrain top 3 models and the ensemble model
top3_models.append('Ensemble')
all_models = top3_model_instances + [('Ensemble', ensemble_model)]

model_metrics = []

for name, model in all_models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    confusion = confusion_matrix(y_test, y_pred)
    cohen_kappa = cohen_kappa_score(y_test, y_pred)
    matthews_corr = matthews_corrcoef(y_test, y_pred)
    log_loss_score = log_loss(y_test, y_pred_proba)

    metrics = {
        'name': name,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'confusion': confusion,
        'cohen_kappa': cohen_kappa,
        'matthews_corr': matthews_corr,
        'log_loss': log_loss_score
    }
    model_metrics.append(metrics)

# Step 4: Take a weighted average of each performance metric from the top 3 models and the ensemble model
# Convert the model_metrics list to a DataFrame
model_metrics_df = pd.DataFrame(model_metrics)

# Display the model_metrics DataFrame
model_metrics_df

